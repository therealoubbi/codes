import numpy as np
import random
from scipy.integrate import odeint
import matplotlib.pyplot as plt
import time

start_time = time.time()

# FitzHugh-Nagumo model
def fitzhugh_nagumo(X, t, a, I_ext, epsilon, gamma):
    v, w = X
    dvdt = v * (v - a) * (1 - v) - w + I_ext
    dwdt = epsilon * (v - gamma * w)
    return [dvdt, dwdt]

# Updated cost function to include both v and w
def cost_function(params, data, t, epsilon, gamma):
    a, I_ext = params  # Both a and I_ext are now estimated
    V0 = data[0, :]  # initial conditions [v0, w0]
    solution = odeint(fitzhugh_nagumo, V0, t, args=(a, I_ext, epsilon, gamma))
    
    # Sum of squared errors for both v and w
    error_v = np.sum((solution[:, 0] - data[:, 0]) ** 2)
    error_w = np.sum((solution[:, 1] - data[:, 1]) ** 2)
    total_error = error_v + error_w
    return total_error

# PSO implementation
class Particle:
    def __init__(self, bounds):
        self.position = [random.uniform(bound[0], bound[1]) for bound in bounds]
        self.velocity = [random.uniform(-1, 1) for _ in bounds]
        self.best_position = self.position.copy()
        self.best_error = float('inf')
        self.error = float('inf')

    def evaluate(self, cost_func, data, t, epsilon, gamma):
        self.error = cost_func(self.position, data, t, epsilon, gamma)
        if self.error < self.best_error:
            self.best_position = self.position.copy()
            self.best_error = self.error

    def update_velocity(self, global_best_position, w=0.5, c1=1.5, c2=1.5):
        for i in range(len(self.position)):
            r1, r2 = random.random(), random.random()
            cognitive_velocity = c1 * r1 * (self.best_position[i] - self.position[i])
            social_velocity = c2 * r2 * (global_best_position[i] - self.position[i])
            self.velocity[i] = w * self.velocity[i] + cognitive_velocity + social_velocity

    def update_position(self, bounds):
        for i in range(len(self.position)):
            self.position[i] += self.velocity[i]
            self.position[i] = max(bounds[i][0], min(self.position[i], bounds[i][1]))

class PSO:
    def __init__(self, cost_func, bounds, num_particles, max_iter, data, t, epsilon, gamma, tol=1e-6, patience=20):
        self.num_particles = num_particles
        self.max_iter = max_iter
        self.bounds = bounds
        self.global_best_position = None
        self.global_best_error = float('inf')
        self.swarm = [Particle(bounds) for _ in range(num_particles)]
        self.cost_func = cost_func
        self.data = data
        self.t = t
        self.epsilon = epsilon
        self.gamma = gamma
        self.tol = tol  # Tolerance for checking if the global best error has changed
        self.patience = patience  # Number of iterations to wait for improvement
        self.no_improvement_count = 0

    def optimize(self):
        prev_global_best_error = self.global_best_error
        
        for i in range(self.max_iter):
            for particle in self.swarm:
                particle.evaluate(self.cost_func, self.data, self.t, self.epsilon, self.gamma)
                if particle.error < self.global_best_error:
                    self.global_best_position = particle.best_position.copy()
                    self.global_best_error = particle.best_error

            for particle in self.swarm:
                particle.update_velocity(self.global_best_position)
                particle.update_position(self.bounds)

            # Check for improvement in global best error
            if abs(self.global_best_error - prev_global_best_error) < self.tol:
                self.no_improvement_count += 1
            else:
                self.no_improvement_count = 0  # Reset if improvement is detected
            
            prev_global_best_error = self.global_best_error

            # Print progress every 10 iterations or at the last iteration
            if (i+1) % 10 == 0 or i == self.max_iter - 1:
                print(f"Iteration {i+1}/{self.max_iter}, Global Best Error: {self.global_best_error}")

            # Early stopping if no improvement for 'patience' iterations
            if self.no_improvement_count >= self.patience:
                print(f"Stopping early at iteration {i+1} due to no improvement in global best error.")
                break

        return self.global_best_position, self.global_best_error

# Example usage
t = np.linspace(0, 50, 500)
true_a = 0.7
gamma = 0.8
epsilon = 0.2
true_I_ext = 0.75
V0 = [0, 0]

# Generate synthetic data (both v and w)
data = odeint(fitzhugh_nagumo, V0, t, args=(true_a, true_I_ext, epsilon, gamma))
observed_data = data + np.random.normal(0, 0.05, data.shape)  # add noise to both v and w

# PSO to tune both a and I_ext using both v and w
bounds = [(0, 1), (0.5, 1)]  # Bounds for both a and I_ext
num_particles = 30
max_iter = 300
pso = PSO(cost_function, bounds, num_particles, max_iter, observed_data, t, epsilon, gamma, tol=1e-19, patience=60)
best_params, best_error = pso.optimize()

print(f"Best parameter a: {best_params[0]}")
print(f"Best parameter I_ext: {best_params[1]}")
print(f"Best error: {best_error}")
end_time = time.time()  # Record the end time
execution_time = end_time - start_time  # Calculate the time difference
print(f"Execution time: {execution_time} seconds")

# Plot results
best_solution = odeint(fitzhugh_nagumo, V0, t, args=(best_params[0], best_params[1], epsilon, gamma))
plt.plot(t, observed_data[:, 0], 'b-', label='v data')
plt.plot(t, observed_data[:, 1], 'g-', label='w data')
plt.plot(t, best_solution[:, 0], 'r--', label='v fit')
plt.plot(t, best_solution[:, 1], 'y--', label='w fit')
plt.xlabel('Time')
plt.ylabel('Variables')
plt.legend()
plt.show()
