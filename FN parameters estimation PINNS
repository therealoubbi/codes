import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from scipy.integrate import odeint
import time

start_time = time.time() 


# True parameters
a_true = 0.7
gamma_true = 0.8
epsilon_true = 0.2
I_ext_true = 0.75

def fhn_ode(y, t, a, gamma, epsilon, I_ext):
    v, w = y
    dv_dt = v * (v - a) * (1 - v) - w + I_ext
    dw_dt = epsilon * (v - gamma * w)
    return [dv_dt, dw_dt]

# Time points for generating data
t_data = np.linspace(0, 100, 200)
y0 = [0.0, 0.0]  # Initial conditions

# Solve the ODE using the true parameters
solution = odeint(fhn_ode, y0, t_data, args=(a_true, gamma_true, epsilon_true, I_ext_true))
v_data = solution[:, 0]
w_data = solution[:, 1]

# Add noise to the data (optional)
noise_level = 0.2 # 5% noise
v_data_noisy = v_data + noise_level * np.std(v_data) * np.random.randn(len(v_data))
w_data_noisy = w_data + noise_level * np.std(w_data) * np.random.randn(len(w_data))

# Convert data to tensors
t_train = torch.tensor(t_data.reshape(-1, 1), dtype=torch.float32, requires_grad=True)
v_train = torch.tensor(v_data_noisy.reshape(-1, 1), dtype=torch.float32)
w_train = torch.tensor(w_data_noisy.reshape(-1, 1), dtype=torch.float32)

# Neural Network Definition with trainable parameters
class PINN_FitzHughNagumo(nn.Module):
    def __init__(self):
        super(PINN_FitzHughNagumo, self).__init__()
        # Neural network layers
        self.hidden1 = nn.Linear(1, 64)
        self.hidden2 = nn.Linear(64, 64)
        self.hidden3 = nn.Linear(64, 64)
        self.output = nn.Linear(64, 2)  # Outputs v and w

        # Initialize parameters to be estimated
        self.a = nn.Parameter(torch.tensor(0.5, dtype=torch.float32))
        self.gamma = nn.Parameter(torch.tensor(0.5, dtype=torch.float32))
        self.epsilon = nn.Parameter(torch.tensor(0.5, dtype=torch.float32))
        self.I_ext = nn.Parameter(torch.tensor(0.5, dtype=torch.float32))

    def forward(self, t):
        x = self.hidden1(t)
        x = torch.sin(x)  # Activation function
        x = self.hidden2(x)
        x = torch.sin(x)
        x = self.hidden3(x)
        x = torch.sin(x)
        out = self.output(x)
        return out

# Instantiate the model and optimizer
model = PINN_FitzHughNagumo()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# Set tolerance for stopping criteria
tolerance = 1e-6
param_tolerance = 1e-20  # Threshold for convergence of parameters
max_epochs = 30000  # Optional: safety stop to avoid infinite loops
loss_history = []

# Initialize previous values for the parameters
prev_a = model.a.item()
prev_gamma = model.gamma.item()
prev_epsilon = model.epsilon.item()
prev_I_ext = model.I_ext.item()

epoch = 0
while True:
    optimizer.zero_grad()
    
    # Predict v and w
    v_w_pred = model(t_train)
    v_pred = v_w_pred[:, 0:1]
    w_pred = v_w_pred[:, 1:2]
    
    # Compute time derivatives
    dv_dt = torch.autograd.grad(v_pred, t_train, grad_outputs=torch.ones_like(v_pred), create_graph=True)[0]
    dw_dt = torch.autograd.grad(w_pred, t_train, grad_outputs=torch.ones_like(w_pred), create_graph=True)[0]
    
    # Retrieve the trainable parameters
    a = model.a
    gamma = model.gamma
    epsilon = model.epsilon
    I_ext = model.I_ext
    
    # Compute residuals of the differential equations
    res_v = dv_dt - (v_pred * (v_pred - a) * (1 - v_pred) - w_pred + I_ext)
    res_w = dw_dt - (epsilon * (v_pred - gamma * w_pred))
    
    # Physics loss
    loss_res_v = torch.mean(res_v ** 2)
    loss_res_w = torch.mean(res_w ** 2)
    loss_phys = loss_res_v + loss_res_w
    
    # Data loss
    loss_data_v = torch.mean((v_pred - v_train) ** 2)
    loss_data_w = torch.mean((w_pred - w_train) ** 2)
    loss_data = loss_data_v + loss_data_w
    
    # Initial condition loss
    t0 = torch.tensor([[0.0]], requires_grad=True)
    v_w0 = model(t0)
    v0_pred = v_w0[:, 0]
    w0_pred = v_w0[:, 1]

    v0_true = torch.tensor([y0[0]])
    w0_true = torch.tensor([y0[1]])

    loss_ic = (v0_pred - v0_true) ** 2 + (w0_pred - w0_true) ** 2
    
    # Total loss
    loss = loss_phys + loss_data + loss_ic
    
    # Backpropagation and optimization
    loss.backward()
    optimizer.step()
    
    # Record loss history
    loss_history.append(loss.item())
    
    # Print loss and parameter estimates every 1000 epochs
    if epoch % 1000 == 0:
        print(f'Epoch {epoch}: Loss = {loss.item():.6f}')
        print(f'Estimated Parameters - a: {a.item():.4f}, gamma: {gamma.item():.4f}, epsilon: {epsilon.item():.4f}, I_ext: {I_ext.item():.4f}')
    
    # Check if loss is below the tolerance
    if loss.item() < tolerance:
        print(f'Training stopped at epoch {epoch} with loss = {loss.item():.6f}')
        break
    
    # Check if the changes in all parameters are below the threshold
    if (
        abs(a.item() - prev_a) < param_tolerance and
        abs(gamma.item() - prev_gamma) < param_tolerance and
        abs(epsilon.item() - prev_epsilon) < param_tolerance and
        abs(I_ext.item() - prev_I_ext) < param_tolerance
    ):
        print(f"All parameters have converged at epoch {epoch}.")
        print(f"Final values - a: {a.item():.6f}, gamma: {gamma.item():.6f}, epsilon: {epsilon.item():.6f}, I_ext: {I_ext.item():.6f}")
        break
    
    # Update previous values for the parameters
    prev_a = a.item()
    prev_gamma = gamma.item()
    prev_epsilon = epsilon.item()
    prev_I_ext = I_ext.item()
    
    # Optionally, stop if max epochs are reached
    if epoch >= max_epochs:
        print(f'Max epochs reached. Final loss = {loss.item():.6f}')
        break
    
    epoch += 1


# Retrieve the estimated parameters
a_est = model.a.item()
gamma_est = model.gamma.item()
epsilon_est = model.epsilon.item()
I_ext_est = model.I_ext.item()

print('\nEstimated Parameters:')
print(f'a: Estimated = {a_est:.4f}, True = {a_true}')
print(f'gamma: Estimated = {gamma_est:.4f}, True = {gamma_true}')
print(f'epsilon: Estimated = {epsilon_est:.4f}, True = {epsilon_true}')
print(f'I_ext: Estimated = {I_ext_est:.4f}, True = {I_ext_true}')

# Time points for evaluation
t_test = np.linspace(0, 100, 500)
t_test_tensor = torch.tensor(t_test.reshape(-1, 1), dtype=torch.float32, requires_grad=True)

# Numerical solution using estimated parameters
solution_est = odeint(fhn_ode, y0, t_test, args=(a_est, gamma_est, epsilon_est, I_ext_est))
v_est = solution_est[:, 0]
w_est = solution_est[:, 1]

# Model predictions
with torch.no_grad():
    v_w_pred_test = model(t_test_tensor)
    v_pred_test = v_w_pred_test[:, 0].numpy()
    w_pred_test = v_w_pred_test[:, 1].numpy()

# Numerical solution using true parameters
solution_true = odeint(fhn_ode, y0, t_test, args=(a_true, gamma_true, epsilon_true, I_ext_true))
v_true = solution_true[:, 0]
w_true = solution_true[:, 1]

end_time = time.time()  # Record the end time
execution_time = end_time - start_time  # Calculate the time difference
print(f"Execution time: {execution_time} seconds")

# Plot the results
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(t_test, v_true, label='True Solution', color='blue')
plt.plot(t_test, v_est, label='Solution with Estimated Parameters', color='green', linestyle='--')
plt.plot(t_test, v_pred_test, label='PINN Prediction', color='red', linestyle=':')
plt.scatter(t_data, v_data_noisy, label='Noisy Data', color='black', s=10, alpha=0.5)
plt.xlabel('Time t')
plt.ylabel('v(t)')
plt.title('Membrane Potential v(t)')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(t_test, w_true, label='True Solution', color='blue')
plt.plot(t_test, w_est, label='Solution with Estimated Parameters', color='green', linestyle='--')
plt.plot(t_test, w_pred_test, label='PINN Prediction', color='red', linestyle=':')
plt.scatter(t_data, w_data_noisy, label='Noisy Data', color='black', s=10, alpha=0.5)
plt.xlabel('Time t')
plt.ylabel('w(t)')
plt.title('Recovery Variable w(t)')
plt.legend()

plt.tight_layout()
plt.show()

# Plot loss history
plt.figure()
plt.plot(loss_history)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss History')
plt.show()
