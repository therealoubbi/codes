import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import time

start_time = time.time() 

# Parameters
a = 0.7
gamma = 0.8
epsilon = 0.05
I_ext = 0.3

# Time points for generating data
t_data = np.linspace(0, 100, 200)
y0 = [0.0, 0.0]  # Initial condition

# Convert data to tensors
t_train = torch.tensor(t_data.reshape(-1, 1), dtype=torch.float32, requires_grad=True)


# Neural Network Definition
# Neural Network Definition
class PINN_FitzHughNagumo(nn.Module):
    def __init__(self):
        super(PINN_FitzHughNagumo, self).__init__()
        self.hidden1 = nn.Linear(1, 64)
        self.hidden2 = nn.Linear(64, 64)
        self.hidden3 = nn.Linear(64, 64)
        self.output = nn.Linear(64, 2)  # Outputs v and w
    
    def forward(self, t):
        x = self.hidden1(t)
        x = torch.sin(x)  # Activation function after hidden1
        x = self.hidden2(x)  # Pass the output of hidden1 to hidden2
        x = torch.sin(x)  # Activation function after hidden2
        x = self.hidden3(x)  # Pass the output of hidden2 to hidden3
        x = torch.sin(x)  # Activation function after hidden3
        out = self.output(x)  # Final output layer
        return out


# Instantiate the model and optimizer
model = PINN_FitzHughNagumo()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# Training Loop
num_epochs = 10
loss_history = []

for epoch in range(num_epochs):
    optimizer.zero_grad()
    
    # Predict v and w
    v_w_pred = model(t_train)
    v_pred = v_w_pred[:, 0:1]
    w_pred = v_w_pred[:, 1:2]
    
    # Compute time derivatives
    dv_dt = torch.autograd.grad(v_pred, t_train, grad_outputs=torch.ones_like(v_pred), create_graph=True)[0]
    dw_dt = torch.autograd.grad(w_pred, t_train, grad_outputs=torch.ones_like(w_pred), create_graph=True)[0]
    
    # Residuals of the differential equations
    res_v = dv_dt - (v_pred * (v_pred - a) * (1 - v_pred) - w_pred + I_ext)
    res_w = dw_dt - (epsilon * (v_pred - gamma * w_pred))
    
    # Physics loss (mean squared error of residuals)
    loss_res_v = torch.mean(res_v ** 2)
    loss_res_w = torch.mean(res_w ** 2)
    loss_phys = loss_res_v + loss_res_w
    



    
    # Initial condition at t = 0
    t0 = torch.tensor([[0.0]], requires_grad=True)
    v_w0 = model(t0)
    v0_pred = v_w0[:, 0]
    w0_pred = v_w0[:, 1]

    v0_true = torch.tensor([y0[0]])
    w0_true = torch.tensor([y0[1]])

    loss_ic = (v0_pred - v0_true) ** 2 + (w0_pred - w0_true) ** 2
    
    # Total loss (weighted sum)
    loss = loss_phys + loss_ic
    
    # Backpropagation and optimization
    loss.backward()
    optimizer.step()
    
    # Record loss history
    loss_history.append(loss.item())
    
    # Print loss every 1000 epochs
    if epoch % 100 == 0:
        print('Epoch {}: Loss = {:.6f}'.format(epoch, loss.item()))

# Time points for evaluation
t_test = np.linspace(0, 100, 200)
t_test_tensor = torch.tensor(t_test.reshape(-1, 1), dtype=torch.float32, requires_grad=True)



# Model predictions
with torch.no_grad():
    v_w_pred_test = model(t_test_tensor)
    v_pred_test = v_w_pred_test[:, 0].numpy()
    w_pred_test = v_w_pred_test[:, 1].numpy()

end_time = time.time()  # Record the end time
execution_time = end_time - start_time  # Calculate the time difference
print(f"Execution time: {execution_time} seconds")    

# Plot the results
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(t_test, v_pred_test, label='PINN Prediction', color='red', linestyle='--')

plt.xlabel('Time t')
plt.ylabel('v(t)')
plt.title('Membrane Potential v(t)')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(t_test, w_pred_test, label='PINN Prediction', color='red', linestyle='--')

plt.xlabel('Time t')
plt.ylabel('w(t)')
plt.title('Recovery Variable w(t)')
plt.legend()

plt.tight_layout()
plt.show()

# Plot loss history
plt.figure()
plt.plot(loss_history)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss History')
plt.show()
