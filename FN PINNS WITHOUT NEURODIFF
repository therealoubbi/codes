import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from scipy.integrate import odeint
import time

start_time = time.time() 

# Parameters
l=3

a = 0.7
gamma = 0.8
epsilon = 0.2
I_ext = 0.75

def fhn_ode(y, t):
    v, w = y
    dv_dt = v * (v - a) * (1 - v) - w + I_ext
    dw_dt = epsilon * (v - gamma * w)
    return [dv_dt, dw_dt]

# Time points for generating data
t_data = np.linspace(0, 100, 200)
y0 = [0.0, 0.0]  # Initial condition

# Solve the ODE
solution = odeint(fhn_ode, y0, t_data)
v_data = solution[:, 0]
w_data = solution[:, 1]

# Add noise to the data (optional)
noise_level = 0.05  # 5% noise
v_data_noisy = v_data
w_data_noisy = w_data 

# Convert data to tensors
t_train = torch.tensor(t_data.reshape(-1, 1), dtype=torch.float32, requires_grad=True)
v_train = torch.tensor(v_data_noisy.reshape(-1, 1), dtype=torch.float32)
w_train = torch.tensor(w_data_noisy.reshape(-1, 1), dtype=torch.float32)

# Neural Network Definition
# Neural Network Definition
class PINN_FitzHughNagumo(nn.Module):
    def __init__(self):
        super(PINN_FitzHughNagumo, self).__init__()
        self.hidden1 = nn.Linear(1, 64)
        self.hidden2 = nn.Linear(64, 64)
        self.hidden3 = nn.Linear(64, 64)
        self.output = nn.Linear(64, 2)  # Outputs v and w
    
    def forward(self, t):
        x = self.hidden1(t)
        x = torch.sin(x)  # Activation function after hidden1
        x = self.hidden2(x)  # Pass the output of hidden1 to hidden2
        x = torch.sin(x)  # Activation function after hidden2
        x = self.hidden3(x)  # Pass the output of hidden2 to hidden3
        x = torch.sin(x)  # Activation function after hidden3
        out = self.output(x)  # Final output layer
        return out


# Instantiate the model and optimizer
model = PINN_FitzHughNagumo()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# Training Loop
num_epochs = 10000
loss_history = []

for epoch in range(num_epochs):
    optimizer.zero_grad()
    
    # Predict v and w
    v_w_pred = model(t_train)
    v_pred = v_w_pred[:, 0:1]
    w_pred = v_w_pred[:, 1:2]
    
    # Compute time derivatives
    dv_dt = torch.autograd.grad(v_pred, t_train, grad_outputs=torch.ones_like(v_pred), create_graph=True)[0]
    dw_dt = torch.autograd.grad(w_pred, t_train, grad_outputs=torch.ones_like(w_pred), create_graph=True)[0]
    
    # Residuals of the differential equations
    res_v = dv_dt - (v_pred * (v_pred - a) * (1 - v_pred) - w_pred + I_ext)
    res_w = dw_dt - (epsilon * (v_pred - gamma * w_pred))
    
    # Physics loss (mean squared error of residuals)
    loss_res_v = torch.mean(res_v ** 2)
    loss_res_w = torch.mean(res_w ** 2)
    loss_phys = loss_res_v + loss_res_w
    
    # Data loss (mean squared error between predicted and observed data)
    loss_data_v = torch.mean((v_pred - v_train) ** 2)
    loss_data_w = torch.mean((w_pred - w_train) ** 2)
    loss_data = loss_data_v + loss_data_w
    
    # Initial condition at t = 0
    t0 = torch.tensor([[0.0]], requires_grad=True)
    v_w0 = model(t0)
    v0_pred = v_w0[:, 0]
    w0_pred = v_w0[:, 1]

    v0_true = torch.tensor([y0[0]])
    w0_true = torch.tensor([y0[1]])

    loss_ic = (v0_pred - v0_true) ** 2 + (w0_pred - w0_true) ** 2
    
    # Total loss (weighted sum)
    loss = loss_phys + loss_data + loss_ic
    
    # Backpropagation and optimization
    loss.backward()
    optimizer.step()
    
    # Record loss history
    loss_history.append(loss.item())
    
    # Print loss every 1000 epochs
    if epoch % 1000 == 0:
        print('Epoch {}: Loss = {:.6f}'.format(epoch, loss.item()))

# Time points for evaluation
t_test = np.linspace(0, 100, 500)
t_test_tensor = torch.tensor(t_test.reshape(-1, 1), dtype=torch.float32, requires_grad=True)

# Numerical solution using odeint
solution_test = odeint(fhn_ode, y0, t_test)
v_true = solution_test[:, 0]
w_true = solution_test[:, 1]

# Model predictions
with torch.no_grad():
    v_w_pred_test = model(t_test_tensor)
    v_pred_test = v_w_pred_test[:, 0].numpy()
    w_pred_test = v_w_pred_test[:, 1].numpy()

end_time = time.time()  # Record the end time
execution_time = end_time - start_time  # Calculate the time difference
print(f"Execution time: {execution_time} seconds")    

# Plot the results
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(t_test, v_true, label='Numerical Solution (odeint)', color='blue')
plt.plot(t_test, v_pred_test, label='PINN Prediction', color='red', linestyle='--')
plt.scatter(t_data, v_data_noisy, label='Noisy Data', color='black', s=10, alpha=0.5)
plt.xlabel('Time t')
plt.ylabel('v(t)')
plt.title('Membrane Potential v(t)')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(t_test, w_true, label='Numerical Solution (odeint)', color='blue')
plt.plot(t_test, w_pred_test, label='PINN Prediction', color='red', linestyle='--')
plt.scatter(t_data, w_data_noisy, label='Noisy Data', color='black', s=10, alpha=0.5)
plt.xlabel('Time t')
plt.ylabel('w(t)')
plt.title('Recovery Variable w(t)')
plt.legend()

plt.tight_layout()
plt.show()

# Plot loss history
plt.figure()
plt.plot(loss_history)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss History')
plt.show()


plt.figure()
plt.plot(t_test, v_true-v_pred_test, label='v_net-v_num,')
plt.plot(t_test, w_true-w_pred_test, label='w_net-w_num')

plt.legend()
plt.show()
